{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "clickstream_path = '../data/Behavioral_sample_for_interview_2021_02_05.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing\n",
    "- lemmatize all words\n",
    "- group events by session (just order, not GROUP BY)\n",
    "- sort events by timestamp within the session\n",
    "- explicitly convert timestamp column to timestamp type (with the assumption that date format is YYYY-MM-DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def normalize(words):\n",
    "    return ' '.join([\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in words.split()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(clickstream_path)\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M')\n",
    "df = df.sort_values(by=['session_id', 'timestamp'])\n",
    "df['term'] = df['term'].astype(str).apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrasession term pairs\n",
    "\n",
    "##### Hypothesis:\n",
    "during the session, most users searching for one item type\n",
    "\n",
    "##### Solution:\n",
    "- remove all click_through events, we are interested only in search queries\n",
    "- get all consecutive term pairs in each session\n",
    "- drop duplicate pairs (within one session)\n",
    "- filter out all pairs where one item is a subset of another\n",
    "- count in how many session term pair accrues (more term co-occurrences more likely they are synonyms)\n",
    "- count threshold could be adjusted according to desired precision and recall\n",
    "\n",
    "##### Not solved problems:\n",
    "- all term pairs are symmetric, and we can't say anything about hierarchy (e.g. (apple, fruit) == (fruit, apple)) or we can assume that user first search category and then enter a more specific term (category -> item)\n",
    "- we will find synonymic phrases, not words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_terms = (\n",
    "    df\n",
    "    [df['action'] == 'search']\n",
    "    .drop_duplicates(['session_id', 'term'])\n",
    "    .groupby('session_id')\n",
    "    ['term'].apply(list)\n",
    ")\n",
    "\n",
    "term_pairs = session_terms.apply(lambda terms: list(nltk.bigrams(terms))).sum()\n",
    "term_pairs = pd.Series(term_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_pairs = term_pairs[\n",
    "    term_pairs.apply(\n",
    "        lambda pair: pair[0] not in pair[1] and pair[1] not in pair[0]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results, sorted by confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kettle', 'toaster'),\n",
       " ('cushion', 'throw'),\n",
       " ('pillow', 'cushion'),\n",
       " ('back pack', 'backpack'),\n",
       " ('toaster', 'kettle'),\n",
       " ('lunchbox', 'lunch box'),\n",
       " ('pop it', 'fidget toy'),\n",
       " ('desk', 'table'),\n",
       " ('pop it', 'fidget'),\n",
       " ('fidget toy', 'pop it'),\n",
       " ('contact', 'book cover'),\n",
       " ('bean bag', 'beanbag'),\n",
       " ('car seat', 'booster seat'),\n",
       " ('tv', 'television'),\n",
       " ('bookcase', 'shelf'),\n",
       " ('table', 'desk'),\n",
       " ('tv unit', 'entertainment unit'),\n",
       " ('barbie', 'lol'),\n",
       " ('fidget', 'pop it'),\n",
       " ('book covering', 'contact'),\n",
       " ('light', 'lamp'),\n",
       " ('car seat', 'booster'),\n",
       " ('book cover', 'back to school'),\n",
       " ('rug', 'mat'),\n",
       " ('beanbag', 'bean bag'),\n",
       " ('maternity', 'nursing'),\n",
       " ('back pack', 'bag'),\n",
       " ('ps5', '64226187'),\n",
       " ('throw', 'cushion'),\n",
       " ('bike pant', 'bike short'),\n",
       " ('desk', 'chair'),\n",
       " ('school bag', 'lunch box'),\n",
       " ('pop it', 'toy'),\n",
       " ('lonsdale', 'filum'),\n",
       " ('trolley', 'cart'),\n",
       " ('book cover', 'contact'),\n",
       " ('wall art', 'print'),\n",
       " ('bather', 'bikini'),\n",
       " ('contact', 'book covering'),\n",
       " ('bookcase', 'bookshelf'),\n",
       " ('wrapping paper', 'gift wrap'),\n",
       " ('gift wrap', 'wrapping paper'),\n",
       " ('bookshelf', 'book shelf'),\n",
       " ('tree hut', 'body scrub'),\n",
       " ('frozen', 'bluey'),\n",
       " ('bin', 'basket'),\n",
       " ('drink bottle', 'lunch box'),\n",
       " ('cushion', 'blanket'),\n",
       " ('love to dream', 'swaddle'),\n",
       " ('cushion', 'pillow')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_pairs.value_counts().index.to_list()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsession term pairs (new session start on click_through event)\n",
    "\n",
    "##### Hypothesis:\n",
    "during a session and before click_through event, most users searching for one item type, click_trough event means user found desired item and looking for something else in the next search queries\n",
    "\n",
    "##### Solution:\n",
    "- calculate the number of click_through events before each event (mark each click_trough event as 1, and  use cumulative sum)\n",
    "- use session_id and clicks_before as a unique identifier for subsession (all actions within one session before search success (click_throught event)) \n",
    "- filter out all click_through events, we are interested only in search queries\n",
    "- get all consecutive term pairs in each subsession\n",
    "- filter out all pairs where one item is a subset of another\n",
    "- count in how many session term pair occurs (more term concurrences more likely they are synonyms)\n",
    "- count threshold could be adjusted according to desired precision and recall\n",
    "\n",
    "##### Not solved problems:\n",
    "- all term pairs are symmetric, and we can't say anything about hierarchy (e.g. (apple, fruit) == (fruit, apple)) or we can assume that user first search category and then enter a more specific term (category -> item)\n",
    "- we will find synonymic phrases, not words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clicks_before'] = (df['action'] == 'click_through').astype(int).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_terms = (\n",
    "    df\n",
    "    [df['action'] == 'search']\n",
    "    .drop_duplicates(['session_id', 'term'])\n",
    "    .groupby(['session_id', 'clicks_before'])\n",
    "    ['term'].apply(list)\n",
    ")\n",
    "\n",
    "term_pairs = pd.Series(\n",
    "    session_terms\n",
    "    .apply(lambda terms: list(nltk.bigrams(terms)))\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "term_pairs = term_pairs[\n",
    "    term_pairs.apply(\n",
    "        lambda pair: pair[0] not in pair[1] and pair[1] not in pair[0]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results, sorted by confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kettle', 'toaster'),\n",
       " ('cushion', 'throw'),\n",
       " ('pillow', 'cushion'),\n",
       " ('back pack', 'backpack'),\n",
       " ('toaster', 'kettle'),\n",
       " ('lunchbox', 'lunch box'),\n",
       " ('pop it', 'fidget toy'),\n",
       " ('desk', 'table'),\n",
       " ('pop it', 'fidget'),\n",
       " ('fidget toy', 'pop it'),\n",
       " ('contact', 'book cover'),\n",
       " ('bean bag', 'beanbag'),\n",
       " ('car seat', 'booster seat'),\n",
       " ('tv', 'television'),\n",
       " ('bookcase', 'shelf'),\n",
       " ('table', 'desk'),\n",
       " ('tv unit', 'entertainment unit'),\n",
       " ('barbie', 'lol'),\n",
       " ('fidget', 'pop it'),\n",
       " ('book covering', 'contact'),\n",
       " ('light', 'lamp'),\n",
       " ('car seat', 'booster'),\n",
       " ('book cover', 'back to school'),\n",
       " ('rug', 'mat'),\n",
       " ('beanbag', 'bean bag'),\n",
       " ('maternity', 'nursing'),\n",
       " ('back pack', 'bag'),\n",
       " ('ps5', '64226187'),\n",
       " ('throw', 'cushion'),\n",
       " ('bike pant', 'bike short'),\n",
       " ('desk', 'chair'),\n",
       " ('school bag', 'lunch box'),\n",
       " ('pop it', 'toy'),\n",
       " ('lonsdale', 'filum'),\n",
       " ('trolley', 'cart'),\n",
       " ('book cover', 'contact'),\n",
       " ('wall art', 'print'),\n",
       " ('bather', 'bikini'),\n",
       " ('contact', 'book covering'),\n",
       " ('bookcase', 'bookshelf'),\n",
       " ('wrapping paper', 'gift wrap'),\n",
       " ('gift wrap', 'wrapping paper'),\n",
       " ('bookshelf', 'book shelf'),\n",
       " ('tree hut', 'body scrub'),\n",
       " ('frozen', 'bluey'),\n",
       " ('bin', 'basket'),\n",
       " ('drink bottle', 'lunch box'),\n",
       " ('cushion', 'blanket'),\n",
       " ('love to dream', 'swaddle'),\n",
       " ('cushion', 'pillow')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_pairs.value_counts().index.to_list()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained word vectorizer\n",
    "\n",
    "##### Hypothesys:\n",
    "Pretrained NLP models such as Word2Vec, Glove, Fasttext will give us good quality synonyms.\n",
    "\n",
    "##### Solution:\n",
    "- get all unique words from search queries\n",
    "- get top similar words for each word\n",
    "- if these words are found in search terms, we should use them as synonyms in our search engine\n",
    "- word similarity threshold could be adjusted according to desired precision and recall\n",
    "\n",
    "##### Not solved problems:\n",
    "- no customer-specific information used\n",
    "- we missed all multy-word search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = set(\n",
    "    df['term']\n",
    "    .drop_duplicates()\n",
    "    .str.split()\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39470/39470 [02:39<00:00, 248.04it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "pairs = []\n",
    "\n",
    "for term in tqdm(df['term'].unique()):\n",
    "    try:\n",
    "        for synonym, score in glove_vectors.most_similar(term, topn=10):\n",
    "            synonym = normalize(synonym)\n",
    "            if (synonym != term) and (synonym in search_words):\n",
    "                pairs.append((synonym, term))\n",
    "                scores.append(score)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "synonym_pairs = pd.DataFrame({\n",
    "    'pair': pairs, \n",
    "    'score': scores}\n",
    ").drop_duplicates(subset=['pair'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first items are mostly numbers and just popular words, so we will skip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hoping', 'hope'),\n",
       " ('changing', 'change'),\n",
       " ('change', 'changing'),\n",
       " ('coaster', 'roller'),\n",
       " ('roller', 'coaster'),\n",
       " ('duffel', 'duffle'),\n",
       " ('expect', 'wo'),\n",
       " ('could', 'be'),\n",
       " ('we', 'our'),\n",
       " ('could', 'to'),\n",
       " ('ipod', 'iphones'),\n",
       " ('only', 'no'),\n",
       " ('i', 'my'),\n",
       " ('900', '500'),\n",
       " ('plane', 'airplane'),\n",
       " ('airplane', 'plane'),\n",
       " ('you', 'can'),\n",
       " ('trouser', 'baggy'),\n",
       " ('grain', 'wheat'),\n",
       " ('warmer', 'cooler'),\n",
       " ('eleven', 'nine'),\n",
       " ('back', 'out'),\n",
       " ('out', 'back'),\n",
       " ('s8', 's9'),\n",
       " ('one', 'five'),\n",
       " ('ca', 'wo'),\n",
       " ('wo', 'ca'),\n",
       " ('enough', 'able'),\n",
       " ('manga', 'anime'),\n",
       " ('anime', 'manga'),\n",
       " ('400', '100'),\n",
       " ('but', 'he'),\n",
       " ('serving', 'serve'),\n",
       " ('serve', 'serving'),\n",
       " ('ran', 'running'),\n",
       " ('be', 'can'),\n",
       " ('can', 'be'),\n",
       " ('would', 'able'),\n",
       " ('europe', 'european'),\n",
       " ('maybe', 'me'),\n",
       " ('might', 'wo'),\n",
       " ('daughter', 'sister'),\n",
       " ('freezer', 'refrigerator'),\n",
       " ('refrigerator', 'freezer'),\n",
       " ('white', 'black'),\n",
       " ('black', 'white'),\n",
       " ('once', 'then'),\n",
       " ('then', 'once'),\n",
       " ('make', 'to'),\n",
       " ('able', 'to')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_pairs.sort_values('score', ascending=False)['pair'].to_list()[450:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential improvements\n",
    "\n",
    "- pairs from word vectorizer looks noisy and could be improved with better model selection (e.g. something trained on retail data for example)\n",
    "- it is worth trying to get some kind of diff from sessions and sub-sessions synonymic phrase pairs and assume it as word synonyms\n",
    "- get more data and train word2vec on search terms to find synonymic words\n",
    "- get more data and train word2vec on sessions (one session is sentence, one search query is a word) to find synonymic phrases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
